{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de datos individuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook lo que se hace es obtener para cada cluster predictivo los valores preparados para el forecasting. Se obtiene la suma del consumo de cada usuario del cluster por día."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/05 13:10:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import udf, collect_list\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "# Configurar el tamaño de la memoria del driver y de los ejecutores\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EDA SmartWater \") \\\n",
    "    .config(\"spark.driver.memory\", \"120g\") \\\n",
    "    .config(\"spark.executor.memory\", \"120g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"120g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.pivotMaxValues\", \"200000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extraccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos con las predicciones de clustering asociadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preds = 'output/data/predictions_ALL_1_V3.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos por el cluster del cual queremos obtener los datos concretos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de predicciones:  0 para el cluster:  4\n"
     ]
    }
   ],
   "source": [
    "predictions_df = extraccion.carga_datos(data_preds, 4, 'output/data/consumption.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos un filtrado de outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = extraccion.filtrado_outliers(predictions_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente procesamos los datos para obtener por el acumulado del consumo de díario por usuario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 126:==================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|       day|serial_number|          sum_value|\n",
      "+----------+-------------+-------------------+\n",
      "|2021-01-01|   13LA062798| 0.4740000000000002|\n",
      "|2021-01-01|   13LA062799| 0.5930000000000003|\n",
      "|2021-01-01|   14CA016872|              0.728|\n",
      "|2021-01-01|   14CA016873|0.34800000000000003|\n",
      "|2021-01-01|   14FA084354|              0.384|\n",
      "|2021-01-01|   14FA084371|0.36500000000000005|\n",
      "|2021-01-01|   14FA084393|0.30300000000000005|\n",
      "|2021-01-01|   14FA084415|0.34500000000000003|\n",
      "|2021-01-01|   14FA084419|0.35400000000000004|\n",
      "|2021-01-01|   14FA084427| 0.3380000000000001|\n",
      "|2021-01-01|   14FA084443|0.31400000000000006|\n",
      "|2021-01-01|   14FA084456|              0.334|\n",
      "|2021-01-01|   14FA084463|0.28400000000000003|\n",
      "|2021-01-01|   14FA084470|0.37300000000000005|\n",
      "|2021-01-01|   14FA084500|0.37800000000000006|\n",
      "|2021-01-01|   14FA084507|0.35300000000000004|\n",
      "|2021-01-01|   14FA084540|0.44400000000000006|\n",
      "|2021-01-01|   14KA079247|0.29000000000000004|\n",
      "|2021-01-01|   14KA079307|0.45000000000000007|\n",
      "|2021-01-01|   14KA079350|0.36500000000000005|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df_filtrado.withColumn(\"day\", F.to_date(\"date\"))\n",
    "\n",
    "# Agrupar por 'day' y 'serial_number' y calcular la media de 'value' solo con valores no nulos\n",
    "df_sum_day = df.groupBy(\"day\", \"serial_number\") \\\n",
    "                .agg(F.sum(\"value\").alias(\"sum_value\")) \\\n",
    "                .orderBy(\"day\", \"serial_number\")\n",
    "df_sum_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente guardamos los datos en un csv para poder utilizarlos para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sum_day.write.csv('data_testing/cluster_sub1_3_sum_V3.csv', header=True, mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
